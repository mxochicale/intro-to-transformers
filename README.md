# WIP :school_satchel: :wrench: Attention is all you need!
> "A transformer is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data." [~wiki](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))  

_‘What I cannot create. I do not understand.’ ~ Richard Feynman (1918 – 1988)_ [:tv:](https://youtu.be/GHOGAomJJjM?t=496) [:link:](https://www.quora.com/What-did-Richard-Feynman-mean-when-he-said-What-I-cannot-create-I-do-not-understand)

## Table of content
1. [Tutorials](tutorials)
2. [References](references)

## Clone repository
The github repository link is 
https://github.com/mxochicale/transformers-tutorials

To clone this repo, you might need to generate your SSH keys as suggested [here](https://github.com/mxochicale/tools/blob/main/github/SSH.md).
You can then clone the repository by typing (or copying) the following line in a terminal at your selected path in your machine:
```
cd && mkdir -p repositories/mxochicale && cd repositories/mxochicale
git clone git@github.com:mxochicale/transformers-tutorials.git
```

## Issues 
If you have questions or have experiment any problems, please [open an issue](https://github.com/mxochicale/transformers-tutorials/issues). 
